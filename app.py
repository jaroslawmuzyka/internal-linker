import streamlit as st
import pandas as pd
import numpy as np
import io
from collections import defaultdict

# Konfiguracja strony
st.set_page_config(page_title="AI Internal Linker", page_icon="ðŸ”—", layout="wide")

# --- Funkcje pomocnicze ---

def parse_embedding(emb_str):
    """Konwertuje ciÄ…g tekstowy '0.123, 0.456...' na wektor numpy."""
    try:
        if isinstance(emb_str, str):
            # Usuwamy ewentualne nawiasy klamrowe czy spacje
            clean_str = emb_str.replace('[', '').replace(']', '').strip()
            return np.fromstring(clean_str, sep=',')
        return np.array([])
    except:
        return np.array([])

def cosine_similarity_matrix(source_vecs, target_vecs):
    """
    Oblicza podobieÅ„stwo cosinusowe miÄ™dzy dwiema macierzami wektorÃ³w.
    Zwraca macierz o wymiarach (len(source), len(target)).
    """
    # Normalizacja wektorÃ³w (L2 norm)
    source_norm = np.linalg.norm(source_vecs, axis=1, keepdims=True)
    target_norm = np.linalg.norm(target_vecs, axis=1, keepdims=True)
    
    # Unikanie dzielenia przez zero
    source_norm[source_norm == 0] = 1
    target_norm[target_norm == 0] = 1
    
    # Obliczenie cosinusa: (A . B) / (|A| * |B|)
    similarity = np.dot(source_vecs, target_vecs.T) / np.dot(source_norm, target_norm.T)
    return similarity

def load_data(uploaded_file):
    """Wczytuje plik segmentu i sprawdza kolumny."""
    try:
        df = pd.read_excel(uploaded_file)
        
        # Mapowanie kolumn (elastyczne podejÅ›cie do nazw)
        required_cols = {
            'address': ['Address', 'URL', 'Adres'],
            'title': ['Title 1', 'Title', 'TytuÅ‚'],
            'h1': ['H1-1', 'H1', 'NagÅ‚Ã³wek 1'],
            'emb': ['Extract embeddings', 'Embedding', 'Vector']
        }
        
        col_map = {}
        for key, possible_names in required_cols.items():
            found = False
            for name in possible_names:
                # Szukamy kolumny zawierajÄ…cej danÄ… nazwÄ™ (case insensitive)
                match = next((c for c in df.columns if name.lower() in c.lower()), None)
                if match:
                    col_map[key] = match
                    found = True
                    break
            if not found:
                return None, f"Brak wymaganej kolumny dla: {key} (szukano: {possible_names})"

        # Filtrowanie i parsowanie
        df = df.dropna(subset=[col_map['address'], col_map['emb']])
        
        # Parsowanie embeddingÃ³w do nowej kolumny
        df['parsed_emb'] = df[col_map['emb']].apply(parse_embedding)
        
        # Usuwanie bÅ‚Ä™dnych embeddingÃ³w (pustych)
        df = df[df['parsed_emb'].apply(lambda x: x.size > 0)]
        
        # Ustandaryzowanie nazw kolumn do dalszej pracy
        clean_df = pd.DataFrame({
            'Address': df[col_map['address']].astype(str).str.strip(),
            'Title': df[col_map['title']].fillna('').astype(str),
            'H1': df[col_map['h1']].fillna('').astype(str),
            'Embedding': df['parsed_emb']
        })
        
        return clean_df, None
        
    except Exception as e:
        return None, str(e)

def load_anchors(uploaded_file):
    """Wczytuje plik anchorÃ³w i zwraca sÅ‚ownik {url: [anchor1, anchor2]}."""
    try:
        df = pd.read_excel(uploaded_file)
        # Szukanie kolumn
        url_col = next((c for c in df.columns if 'url' in c.lower()), None)
        anc_col = next((c for c in df.columns if 'anchor' in c.lower()), None)
        
        if not url_col or not anc_col:
            return None, "Nie znaleziono kolumn 'URL' lub 'anchor' w pliku anchorÃ³w."
            
        anchors_map = defaultdict(list)
        for _, row in df.iterrows():
            u = str(row[url_col]).strip()
            a = str(row[anc_col]).strip()
            if u and a:
                anchors_map[u].append(a)
        
        return anchors_map, None
    except Exception as e:
        return None, str(e)

# --- Interfejs UÅ¼ytkownika ---

st.title("ðŸ”— AI Internal Linking Strategy")
st.markdown("""
To narzÄ™dzie generuje strategiÄ™ linkowania wewnÄ™trznego na podstawie **podobieÅ„stwa semantycznego (embeddingÃ³w)**.
Wgraj pliki z segmentami (np. Kategorie, Blog), a AI dobierze najbardziej pasujÄ…ce podstrony.
""")

with st.expander("â„¹ï¸ Instrukcja i format plikÃ³w"):
    st.markdown("""
    1. **Pliki segmentÃ³w (.xlsx):** MuszÄ… zawieraÄ‡ kolumny: `Address`, `Title 1`, `H1-1`, `Extract embeddings`.
    2. **Plik anchorÃ³w (opcjonalny .xlsx):** Kolumny: `URL`, `anchor`.
    3. **DziaÅ‚anie:** Wybierz segment gÅ‚Ã³wny (ÅºrÃ³dÅ‚o linkÃ³w) i segmenty docelowe. System znajdzie najlepsze dopasowania.
    """)

# --- Krok 1: Konfiguracja ---

col1, col2 = st.columns(2)
with col1:
    num_segments = st.number_input("Liczba segmentÃ³w (grup stron)", min_value=2, max_value=10, value=2)
with col2:
    limit_suggestions = st.number_input("Liczba linkÃ³w na artykuÅ‚", min_value=1, max_value=20, value=3)

# Przechowywanie wgranych plikÃ³w w sesji (aby nie znikaÅ‚y przy przeÅ‚adowaniu)
if 'segment_files' not in st.session_state:
    st.session_state['segment_files'] = {}

st.subheader("ðŸ“‚ Wgraj pliki segmentÃ³w")

segments_data = [] # Lista sÅ‚ownikÃ³w: {'name': str, 'df': DataFrame}
has_errors = False

for i in range(num_segments):
    c1, c2 = st.columns([1, 2])
    with c1:
        seg_name = st.text_input(f"Nazwa segmentu {i+1}", value=f"Segment {i+1}", key=f"name_{i}")
    with c2:
        seg_file = st.file_uploader(f"Plik dla: {seg_name}", type=['xlsx'], key=f"file_{i}")
    
    if seg_file:
        df, error = load_data(seg_file)
        if error:
            st.error(f"BÅ‚Ä…d w pliku '{seg_name}': {error}")
            has_errors = True
        else:
            segments_data.append({'name': seg_name, 'df': df})
            st.success(f"âœ… Wczytano {len(df)} adresÃ³w.")

st.subheader("âš“ Anchory (Opcjonalne)")
anchor_file = st.file_uploader("Plik z anchorami (.xlsx)", type=['xlsx'])
anchors_map = {}
if anchor_file:
    a_map, a_err = load_anchors(anchor_file)
    if a_err:
        st.error(f"BÅ‚Ä…d anchorÃ³w: {a_err}")
    else:
        anchors_map = a_map
        st.success(f"âœ… Wczytano anchory dla {len(anchors_map)} adresÃ³w URL.")

# --- Krok 2: Uruchomienie ---

if len(segments_data) == num_segments and not has_errors:
    st.divider()
    
    # WybÃ³r segmentu gÅ‚Ã³wnego
    segment_names = [s['name'] for s in segments_data]
    main_seg_idx = st.selectbox("Wybierz Segment GÅ‚Ã³wny (skÄ…d linkujemy?):", range(len(segment_names)), format_func=lambda x: segment_names[x])
    
    if st.button("ðŸš€ Generuj StrategiÄ™ Linkowania", type="primary"):
        with st.spinner("AnalizujÄ™ wektory i obliczam podobieÅ„stwo..."):
            
            main_segment = segments_data[main_seg_idx]
            other_segments = [s for i, s in enumerate(segments_data) if i != main_seg_idx]
            
            # Przygotowanie macierzy wektorÃ³w dla segmentu gÅ‚Ã³wnego
            # Stackujemy wektory do macierzy numpy (N x Dymensje)
            main_vecs = np.vstack(main_segment['df']['Embedding'].values)
            
            results = []
            usage_counts = defaultdict(int) # Do rotacji anchorÃ³w: {url_target: count}
            
            # Zbieramy wszystkie URL z "innych" segmentÃ³w, aby znaleÅºÄ‡ nielinkowane
            all_target_urls = set()
            linked_target_urls = set()
            
            # GÅ‚Ã³wna pÄ™tla przetwarzania
            # Iterujemy po innych segmentach
            for target_seg in other_segments:
                target_df = target_seg['df']
                target_vecs = np.vstack(target_df['Embedding'].values)
                
                # Dodajemy do puli wszystkich URLi
                all_target_urls.update(target_df['Address'].tolist())
                
                # Obliczamy podobieÅ„stwo WSZYSTKO vs WSZYSTKO dla tej pary segmentÃ³w
                # Wynik to macierz: wiersze = main_urls, kolumny = target_urls
                sim_matrix = cosine_similarity_matrix(main_vecs, target_vecs)
                
                # Dla kaÅ¼dego adresu z Main Segment
                for idx, source_row in main_segment['df'].iterrows():
                    # Pobieramy wiersz podobieÅ„stw dla tego adresu
                    # idx moÅ¼e nie odpowiadaÄ‡ indeksowi macierzy jeÅ›li df ma luki w indexie,
                    # wiÄ™c bezpieczniej uÅ¼yÄ‡ iloc/reset_index, ale tutaj iterujemy po kolei
                    # UÅ¼yjmy licznika pÄ™tli
                    pass 

                # PodejÅ›cie ziterowane po macierzy jest szybsze
                # sim_matrix[i] to podobieÅ„stwa dla i-tego adresu z main_segment
                
                for i in range(len(main_segment['df'])):
                    source_url = main_segment['df'].iloc[i]['Address']
                    
                    # Sortujemy wyniki dla tego wiersza (malejÄ…co)
                    # argsort zwraca indeksy posortowane rosnÄ…co, wiÄ™c bierzemy od tyÅ‚u
                    scores = sim_matrix[i]
                    best_indices = np.argsort(scores)[::-1]
                    
                    # Bierzemy top N, pomijajÄ…c ten sam URL (autolinkowanie)
                    count = 0
                    for target_idx in best_indices:
                        if count >= limit_suggestions:
                            break
                        
                        target_row = target_df.iloc[target_idx]
                        target_url = target_row['Address']
                        score = scores[target_idx]
                        
                        # Pomijamy autolinkowanie
                        if source_url == target_url:
                            continue
                            
                        # --- Logika AnchorÃ³w ---
                        # Sprawdzamy czy mamy zdefiniowane anchory dla TARGET URL
                        target_anchors = anchors_map.get(target_url, [])
                        
                        if target_anchors:
                            # Rotacja anchorÃ³w
                            anchor_idx = usage_counts[target_url] % len(target_anchors)
                            chosen_anchor = target_anchors[anchor_idx]
                        else:
                            chosen_anchor = "BRAK ANCHORA (UÅ¼yj tytuÅ‚u)" # lub puste
                        
                        # ZwiÄ™kszamy licznik uÅ¼ycia targetu
                        usage_counts[target_url] += 1
                        linked_target_urls.add(target_url)
                        
                        results.append({
                            'URL Å¹rÃ³dÅ‚owy': source_url,
                            'Linkuje do': target_url,
                            'Segment Docelowy': target_seg['name'],
                            'Score': round(score, 4),
                            'Anchor': chosen_anchor,
                            'TytuÅ‚ Docelowy': target_row['Title']
                        })
                        count += 1

            # --- Generowanie wynikÃ³w ---
            results_df = pd.DataFrame(results)
            
            # --- Generowanie nielinkowanych ---
            unlinked_list = list(all_target_urls - linked_target_urls)
            unlinked_df = pd.DataFrame(unlinked_list, columns=['Nielinkowany URL'])
            
            # WyÅ›wietlanie
            st.success("âœ… Analiza zakoÅ„czona!")
            
            tab1, tab2 = st.tabs(["ðŸ“Š Propozycje Linkowania", "ðŸš« Nielinkowane Adresy"])
            
            with tab1:
                st.dataframe(results_df, use_container_width=True)
                
                # Eksport
                buffer = io.BytesIO()
                with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                    results_df.to_excel(writer, index=False, sheet_name='Linki')
                
                st.download_button(
                    label="ðŸ“¥ Pobierz strategiÄ™ (.xlsx)",
                    data=buffer.getvalue(),
                    file_name="strategia_linkowania.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
                
            with tab2:
                st.write(f"Znaleziono {len(unlinked_df)} adresÃ³w, ktÃ³re nie otrzymaÅ‚y Å¼adnego linku.")
                st.dataframe(unlinked_df, use_container_width=True)
                
                buffer_un = io.BytesIO()
                with pd.ExcelWriter(buffer_un, engine='xlsxwriter') as writer:
                    unlinked_df.to_excel(writer, index=False, sheet_name='Nielinkowane')
                    
                st.download_button(
                    label="ðŸ“¥ Pobierz nielinkowane (.xlsx)",
                    data=buffer_un.getvalue(),
                    file_name="nielinkowane_urls.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

elif len(segments_data) < num_segments:
    st.info("Wgraj wszystkie wymagane pliki segmentÃ³w, aby rozpoczÄ…Ä‡.")
